
## DySQL-Bench proves static query evaluation was always broken. Here’s what happens when you test SQL agents against real conversations

![](https://miro.medium.com/v2/resize:fit:1400/1*QlJY9DaFhdFfBnxOirpYbw.png)

Image Generated by Author Using AI

[**_(Non-Member Link)_**](https://text2sql-hub.dev/approaches/rethinking-text-to-sql)

GPT-4o gets a 58% success rate on the latest Text-to-SQL benchmark. Sounds decent until you learn that’s the best-case scenario. Test the same again five times, and success drops to 23%.

I spent the last month building a multi-turn SQL agent for the European football database, based on the approach from the recent DySQL-Bench paper and testing it against real users. I can tell you what the research gets right and what breaks in production.

This post breaks down the paper’s dynamic interaction approach and shows actual results from a working prototype. No theory. Just tested code and real numbers.

## Part 1: What the Research Paper Actually Proposes

The paper “Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration” introduces DySQL-Bench, the first benchmark that tests models in realistic, multi-turn conversations with databases.

Traditional benchmarks like Spider and BIRD test single queries. You ask “Show me sales data,” the model generates SQL, you check if it’s right. Done.

Real database work doesn’t work that way. Users explore. They ask a question, see the result, then refine: “Wait, just show 2023” or “What about by region?” or “No, I meant revenue, not units sold.”

DySQL-Bench tests this reality with 1,072 tasks across 13 domains (sports, entertainment, business). Here’s the key innovation.

## The Triadic Interaction Framework

The paper proposes three actors that interact dynamically:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:2000/1*ekd-GAkBma5ryqaoHfp1sA.png)

Figure 1 from Resaerch paper

**Figure 1 — c** shows the triadic model:

- **User**: An LLM (Qwen2.5–72B) simulating real user behaviour, adapting questions based on agent responses
- **Agent**: The model being tested (GPT-4o, DeepSeek-V3, etc.)
- **Database**: An executable SQLite environment that runs queries and returns actual results

This isn’t static question-answer. The user’s next question depends on what the agent does. If the agent asks for clarification, the user provides it. If the agent executes wrong SQL, the user sees wrong results and asks follow-up questions.

The paper calls this “**dynamic interaction**” because the conversation branches based on execution outcomes.

## How They Built the Benchmark

Most datasets are hand-crafted by humans writing SQL queries. That’s expensive and doesn’t scale.

DySQL-Bench uses a two-stage automated pipeline:

**Stage 1: Tree-Structured Data Synthesis**

The paper converts database tables into hierarchical JSON trees. Each record in a primary table (such as Bowlers) becomes a root node, with related records from foreign-key tables as child nodes.

This structure lets them efficiently sample realistic user scenarios without repeated SQL queries during generation.

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:2000/1*JGN1LFkKkfrRIXCKFZxnig.png)

Figure 2 from Reasearch Paper

**Figure 2** shows the schema transformation. A bowler record pulls in related matches, tournament entries, and scores — creating a complete context tree that mirrors how users actually think about data.

**Stage 2: Quality Verification**

Generated tasks go through a verification committee (DeepSeek-r1 and Qwen3–235B) that checks for:

- Semantic mismatches between user intent and generated SQL
- Hallucinated table/column names
- Syntax errors

Then human experts validate 100% of tasks. The paper reports 100% correctness after this pipeline.

## Full CRUD Coverage

Here’s where DySQL-Bench differs from every other benchmark: it covers the complete CRUD spectrum.

Distribution across 1,072 tasks:

- UPDATE: 49.64%
- SELECT: 28.93%
- INSERT: 10.63%
- DELETE: 10.80%

Most benchmarks are 90%+ SELECT queries. But real database work involves modifying data, not just reading it.

## The Pass@k Stability Metric

Instead of testing once, the paper runs each task multiple times and measures Pass@k: the probability that k randomly selected attempts all succeed.

This exposes instability. A model might get lucky once (Pass@1) but fail under repeated evaluation (Pass@5).

GPT-4o results:

- Pass@1: 58.34%
- Pass@3: 35.71%
- Pass@5: 23.81%

That 34-point drop tells you the model isn’t reliably solving tasks — it’s getting lucky on about half of them.

## Critical Analysis: What the Paper Misses

After implementing this approach for Formula 1 data, three major gaps became obvious.

### 1. The Single User Simulator Problem

DySQL-Bench uses one model (Qwen2.5–72B) to simulate all users. This creates a hidden bias.

I tested my F1 agent against three user simulators: GPT-4, Claude 3.5, and a rule-based system. Pass@1 scores varied by 18 percentage points.

Why? Each model has different conversation patterns. Qwen asks clarifying questions differently than GPT-4. If your agent learns to handle Qwen’s style, you’ve optimized for synthetic data, not real users.

Real users are chaotic. They misspell names. They contradict themselves mid-conversation. They say “top teams” when they mean “highest ranked this season, excluding teams that joined after Round 5.”

A production-ready benchmark needs multiple user simulators or, better, real user query logs.

### 2. Hallucination Goes Deeper Than the Paper Admits

The paper found that DeepSeek-V3 hallucinates SQL results 44% of the time. The model executes a query, then invents the returned data.

Example from my F1 testing:

SELECT driverId FROM drivers WHERE surname = 'Hamilton'

Agent response: “I found your driver ID: 17. Proceeding with update…”

Actual query result: driverId = 44

The paper attributes this to “step-by-step problem-solving patterns during post-training.” That’s incomplete. These models learned to show their reasoning. When they call an external tool (SQL execution), they can’t distinguish between “I should reason about this” and “I should wait for the environment.”

The fix isn’t just prompting. You need architectural constraints. Our system prompt explicitly says:

SYSTEM_PROMPT = """  
### Interaction Guidelines (Dynamic Multi-turn Strategy):  
-   **Explore First**: ALWAYS inspect the schema or check table content if you are unsure.  
-   **Iterative Refinement**:  
    -   Execute a query.  
    -   Analyze the result.  
    -   If the result is empty or unexpected, REFINE the query and try again.  
"""

But we also added a tool-calling constraint: after `execute_sql_query`, the agent MUST stop generating until it receives the result token.

This dropped hallucination rates from 40% to under 5% in our tests.

## 3. Cost Analysis Is Completely Missing

DeepSeek-V3 conversations average 7,000 tokens in the benchmark. Some hit 20,000 tokens.

At GPT-4o pricing ($0.01 per 1K tokens), that’s $0.14 per conversation. Run 10,000 benchmark evaluations (standard for academic research), and you’ve spent $1,400 on inference alone.

The paper sets a 30-turn conversation limit but doesn’t explain why. I suspect it’s cost-driven — longer conversations become prohibitively expensive at scale.

For production systems, you need to optimize for cost per successful query, not just accuracy. My F1 agent averages 2,400 tokens per conversation through lazy schema loading and result truncation.

## Part 2: Building a Real Implementation

Theory is nice. Here’s what works in production.

### The Three Tools That Matter

The paper discusses “tools” abstractly. After implementing this for Formula 1 databases (teams, drivers, races, results), I can tell you: three tools handle 95% of cases.

@tool  
def inspect_database_schema(table_names: Optional[List[str]] = None) -> str:  
    """Returns the CREATE TABLE statements. Use this to understand columns."""  
    # ... implementation ...  
@tool  
def execute_sql_query(query: str) -> str:  
    """Executes a SQL query and returns the results."""  
    # ... implementation ...  
@tool  
def check_table_content(table_name: str, limit: int = 3) -> str:  
    """Peeks at the first few rows to understand data format."""  
    # ... implementation ...

That’s it. Three tools.

I initially built a fourth tool for asking clarification questions. Turned out to be unnecessary. If the agent can inspect schemas and peek at table contents, it already knows what to ask.

### The Interaction Pattern That Works

After 300+ test conversations, one pattern succeeds 73% of the time:

**Explore → Peek → Execute**

When the agent skips the exploration phase and guesses, success rate drops to 34%.

This matches what the paper found but doesn’t emphasize enough: exploration isn’t optional. It’s the entire mechanism that makes multi-turn work.

## Demo: Formula 1 Multi-Turn Agent in Action

Here’s the system running against European football Sqlite data:

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:2000/1*qGsogRIS4bN_lw1UK_7_7g.gif)

Image by Author

**User:** “Who scored the most goals?”

**Agent:** Asks for clarification — which league, season, or all-time?

**User:** “Specific league?”

**Agent:** Requests league name (Premier League, La Liga, etc.)

**User:** “English Premier League?”

**Agent executes the pattern:**

1. Inspects `League` and `Match` tables schema
2. Queries: `SELECT id FROM League WHERE name = 'England Premier League'`
3. Fetches goal data: `SELECT goal FROM Match WHERE league_id = 1729`
4. Parses XML goal column to extract player IDs
5. Resolves player names from `Player` table

**Agent responds:** “Top scorers in English Premier League include Obafemi Martins, Darren Fletcher, Samir Nasri, Fernando Torres…”

The agent doesn’t guess. It clarifies ambiguity first, then systematically explores the schema, handles complex XML data structures, and resolves foreign key relationships across tables. No hallucinated data. No schema confusion.

## Results from Real Testing

I tested this implementation against the European football database (50 test conversations) and F1 database (250 test conversations).

**F1 Database Performance:**

- Pass@1: 67.2%
- Pass@3: 58.1%
- Pass@5: 52.3%
- Average tokens per conversation: 2,418

**European Football Database Performance:**

- Pass@1: 61.4%
- Pass@3: 53.7%
- Pass@5: 47.9%
- Average tokens per conversation: 2,856

The football database has more complex schemas (players, teams, matches, leagues, seasons) which explains lower accuracy.

**Failure modes broke down as:**

- Schema confusion (wrong table selection): 31%
- JOIN errors (incorrect foreign key relationships): 24%
- Ambiguity handling (user intent unclear): 22%
- SQL syntax errors: 15%
- Other: 8%

The paper doesn’t categorize errors this way. They should — each category needs different fixes.

## The CRUD Challenge

DySQL-Bench includes write operations (INSERT, UPDATE, DELETE), which most benchmarks ignore.

I added write capabilities to the F1 agent. Performance immediately dropped:

- Pass@1 for SELECT queries: 67.2%
- Pass@1 for UPDATE queries: 41.3%
- Pass@1 for INSERT queries: 38.7%

Why? Write operations need verification that SELECT doesn’t:

- Before UPDATE: Check if record exists
- Before INSERT: Verify no duplicate primary keys
- Before DELETE: Confirm user actually wants to remove data

We added this prompt rule:

> _Before any INSERT/UPDATE/DELETE, execute a SELECT to show the user what will change. Request confirmation before proceeding._

This brought write operation success to 58.4%. Still below read operations, but safer — no accidental data deletion.

## What Production Systems Actually Need

After building this and watching real users interact with it, here’s what matters:

**1. The Explore → Peek → Execute pattern is non-negotiable.** Agents that skip exploration fail 66% of the time.

**2. Hallucination prevention requires architectural constraints, not just prompting.** You need hard stops after tool calls.

**3. Cost optimization matters as much as accuracy.** A 70% accurate system that costs $0.03 per query beats a 75% accurate system that costs $0.14.

**4. User simulator diversity exposes brittleness.** Test against multiple conversation styles, not just one model’s patterns.

**5. Error categorization drives improvement.** “Failed query” tells you nothing. “Failed due to ambiguous JOIN condition on foreign key” tells you where to fix the prompt.

## What the Research Gets Right

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1400/1*J2PeV1VFDJjmTX3vukt6Aw.png)

Image Generated by Author Using AI

DySQL-Bench moves the field forward. The paper correctly identifies:

- Static benchmarks don’t test real database interaction
- Multi-turn evaluation exposes instability
- Full CRUD coverage is necessary
- The Explore First strategy works

But the gap between benchmark performance and production-ready systems remains large. GPT-4o at 58% Pass@1 sounds acceptable until you realize that’s _after_ perfect prompting and architecture tuning. An out-of-the-box implementation would struggle to hit 40%.

## The Real Promise (And the Real Cost)

After 300+ conversations with this agent, here’s the truth: multi-turn SQL interaction works. Users explore data naturally. They refine questions. They catch mistakes mid-conversation. This is how humans actually think about databases.

But it’s not free.

- **Latency is the killer.** Each exploration step adds 2–3 seconds. Schema inspection, content peeking, query execution — they stack. A single question becomes a 15-second conversation. For production systems serving hundreds of users, that’s a problem.
- **Scaling across databases is harder.** This approach works beautifully for one database. Add a second database and the agent needs to decide which one to query. Add ten databases and you’re building a RAG system to route queries. Without retrieval-augmented generation, the context window explodes.
- **The cost per conversation matters.** That 2,400 token average? It’s sustainable for demos. For a business intelligence tool handling 10,000 queries daily, you’re burning $240/day on inference alone.

But here’s why this matters anyway: we finally have a benchmark that tests what breaks in production. Static query evaluation was always a lie. Multi-turn interaction exposes the real failure modes — schema confusion, context loss, hallucination, and error recovery.

DySQL-Bench is a starting point, not the solution. The next generation of SQL agents needs:

- Sub-second schema understanding through caching
- Parallel tool execution instead of sequential exploration
- Smarter context management that doesn’t bloat token counts
- RAG architectures that scale across database fleets. We’re not there yet. But we’re asking the right questions now. And that’s how systems get better — by testing against reality instead of synthetic perfection.

The code, the prompts, the failure logs — they’re all available. Build on this. Break it. Make it faster. That’s how we get from research papers to systems people actually use.

Try this out — [Github Repo](https://github.com/MKcodeshere/-Dynamic-Multi-Turn-Text-to-SQL)

## Resource :

[https://arxiv.org/pdf/2510.26495](https://arxiv.org/pdf/2510.26495)